{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5-6 Ball_per_over Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({6: 2888})\n"
     ]
    }
   ],
   "source": [
    "ball_per_over=[]\n",
    "\n",
    "folder_path = \"../data/raw/cricsheet/all_json/\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path,'r') as file:\n",
    "        data=json.load(file)\n",
    "        ball_per_over.append(data[\"info\"][\"balls_per_over\"])\n",
    "\n",
    "\n",
    "counter = Counter(ball_per_over)\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_venue=[]\n",
    "\n",
    "folder_path = \"../data/raw/cricsheet/all_json/\"\n",
    "\n",
    "venue_matches={}\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    with open(file_path,'r') as file:\n",
    "        data=json.load(file)\n",
    "        match_id=filename\n",
    "        # print(match_id)\n",
    "        # print(file)\n",
    "        if \"venue\" in data[\"info\"]:\n",
    "            field_venue.append(data[\"info\"][\"venue\"])\n",
    "            # print(field_venue)\n",
    "            if(data[\"info\"][\"venue\"] in venue_matches):\n",
    "                venue_matches[data[\"info\"][\"venue\"]].append(filename)\n",
    "            else:\n",
    "                venue_matches[data[\"info\"][\"venue\"]]=[filename]\n",
    "            # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique stadiums :367\n"
     ]
    }
   ],
   "source": [
    "print(f'Unique stadiums :{len(venue_matches.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing all the stadiums\n",
    "with open(\"pitch_name.txt\", \"w\") as file:\n",
    "    for pitch in field_venue:\n",
    "        file.write(pitch+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data/raw/additional_data/player_data.csv\"\n",
    "player_data_df=pd.read_csv(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>full_name</th>\n",
       "      <th>batting_style</th>\n",
       "      <th>bowling_style</th>\n",
       "      <th>playing_role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bc005f0d</td>\n",
       "      <td>Aneka Akeilia Aresha White</td>\n",
       "      <td>Right hand Bat</td>\n",
       "      <td>Right arm Medium</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5c67c7a6</td>\n",
       "      <td>Azam Ali Baig</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6a58e069</td>\n",
       "      <td>Arun Bamal</td>\n",
       "      <td>Left hand Bat</td>\n",
       "      <td>Slow Left arm Orthodox</td>\n",
       "      <td>Bowling Allrounder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b4a23876</td>\n",
       "      <td>Alex Adrian Anthony Amsterdam</td>\n",
       "      <td>Left hand Bat</td>\n",
       "      <td>Right arm Offbreak</td>\n",
       "      <td>Top order Batter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>482762af</td>\n",
       "      <td>Adewale A Adeoye</td>\n",
       "      <td>Right hand Bat</td>\n",
       "      <td>Right arm Medium</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  identifier                      full_name   batting_style  \\\n",
       "0   bc005f0d     Aneka Akeilia Aresha White  Right hand Bat   \n",
       "1   5c67c7a6                  Azam Ali Baig             NaN   \n",
       "2   6a58e069                     Arun Bamal   Left hand Bat   \n",
       "3   b4a23876  Alex Adrian Anthony Amsterdam   Left hand Bat   \n",
       "4   482762af               Adewale A Adeoye  Right hand Bat   \n",
       "\n",
       "            bowling_style        playing_role  \n",
       "0        Right arm Medium                 NaN  \n",
       "1                     NaN                 NaN  \n",
       "2  Slow Left arm Orthodox  Bowling Allrounder  \n",
       "3      Right arm Offbreak    Top order Batter  \n",
       "4        Right arm Medium                 NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Right hand Bat', nan, 'Left hand Bat'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_data_df[\"batting_style\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_data_df[\"bowling_style\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the target URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix successfully written to output.csv\n"
     ]
    }
   ],
   "source": [
    "final_data=[]\n",
    "name=list(set(field_venue))\n",
    "for ele in name:\n",
    "    try:\n",
    "        url = \"https://pitch-report.com/?s=\"+ele  # Replace with the actual URL\n",
    "    # temp=[]\n",
    "    # Send an HTTP request to fetch the page content\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if ele in a.get_text()]\n",
    "            response = requests.get(links[0])\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                keywords=[\"Boundary\",\"Field\",\"Surface\"]\n",
    "                bounday_data=[li.get_text() for li in  soup.find_all(\"li\") if any(keyword in li.getText() for keyword in keywords)]\n",
    "\n",
    "                divs = soup.find_all(\"h3\")\n",
    "                div=[div for div in divs if \"Batting Pitch Or Bowling Pitch\" in div.get_text(strip=True)]\n",
    "\n",
    "                d=div[0].find_parent(\"div\").find_parent(\"div\").find_parent(\"div\").find_parent(\"div\")\n",
    "                d=d.find_next_sibling()\n",
    "                data=[p.get_text() for p in d.find_all('p')]\n",
    "                bounday_data.append(data[0])\n",
    "                temp_data=\" \".join(data)\n",
    "                bounday_data.append(temp_data)\n",
    "                bounday_data.append(ele)\n",
    "                final_data.append(bounday_data)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "file_path = \"output.csv\"\n",
    "try:\n",
    "    with open(file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the rows of the matrix into the CSV\n",
    "        writer.writerows(final_data)\n",
    "        \n",
    "    print(f\"Matrix successfully written to {file_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extracting the type of stadium and other info about the stadium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Field size:\\xa0', 'Surface:\\xa0', 'Boundary Length (Dimension):\\xa0', 'Leg Side Boundary Length:\\xa0', 'Off Side Boundary Length:', 'Straight Boundary Length:', 'Back Side Boundary Length:', 'Batting or Bowling Pitch']\n"
     ]
    }
   ],
   "source": [
    "final_data=[]\n",
    "name=list(set(field_venue))\n",
    "for ele in name:\n",
    "    try:\n",
    "        url = \"https://pitch-report.com/?s=\"+ele  # Replace with the actual URL\n",
    "    # temp=[]\n",
    "    # Send an HTTP request to fetch the page content\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if ele in a.get_text()]\n",
    "            response = requests.get(links[0])\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                keywords=[\"Boundary\",\"Field\",\"Surface\"]\n",
    "                bounday_data=[li.get_text() for li in  soup.find_all(\"li\") if any(keyword in li.getText() for keyword in keywords)]\n",
    "\n",
    "                divs = soup.find_all(\"h3\")\n",
    "                div=[div for div in divs if \"Batting Pitch Or Bowling Pitch\" in div.get_text(strip=True)]\n",
    "\n",
    "                d=div[0].find_parent(\"div\").find_parent(\"div\").find_parent(\"div\").find_parent(\"div\")\n",
    "                d=d.find_next_sibling()\n",
    "                data=[p.get_text() for p in d.find_all('p')]\n",
    "                bounday_data.append(data[0])\n",
    "              \n",
    "                temp_data=\" \".join(data)\n",
    "                bounday_data.append(temp_data)\n",
    "                bounday_data.append(ele)\n",
    "                final_data.append(bounday_data)\n",
    "                # print(final_data)\n",
    "                # print(bounday_data)\n",
    "    except:\n",
    "        pass\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from spacy) (2.2.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: jinja2 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ai21btech11012/miniconda3/envs/dream11/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.2/29.2 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, mdurl, marisa-trie, cloudpathlib, click, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 click-8.1.8 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 preshed-3.0.9 pydantic-2.10.6 pydantic-core-2.27.2 rich-13.9.4 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['57%', '43%']\n",
      "Extracted Percentages: ['65%', '35%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['30%', '70%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['29%', '71%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['40%', '60%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['55%', '45%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['53%', '47%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['40%', '60%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['33%', '67%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['54%', '46%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['65%', '35%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['40%', '60%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['67%', '33%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['56%', '44%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['55%', '45%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['72%', '28%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['67%', '33%']\n",
      "Extracted Percentages: ['43%', '57%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['40%', '60%']\n",
      "Extracted Percentages: ['30%', '70%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['67%', '33%']\n",
      "Extracted Percentages: ['45%', '55%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['51%', '49%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['45%', '55%']\n",
      "Extracted Percentages: ['53%', '47%']\n",
      "Extracted Percentages: ['67%', '28%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['31%', '69%']\n",
      "Extracted Percentages: ['33%', '67%']\n",
      "Extracted Percentages: ['20%', '80%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['30%', '70%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['55%', '45%']\n",
      "Extracted Percentages: ['55%', '45%']\n",
      "Extracted Percentages: ['40%', '60%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['80%', '20%']\n",
      "Extracted Percentages: ['40%', '60%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['40%', '60%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['67%', '28%']\n",
      "Extracted Percentages: ['30%', '70%']\n",
      "Extracted Percentages: ['43%', '57%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['45%', '55%']\n",
      "Extracted Percentages: ['55%', '45%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['40%', '60%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['30%', '70%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['40%', '60%']\n",
      "Extracted Percentages: ['65%', '35%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['65%', '35%']\n",
      "Extracted Percentages: ['73%', '27%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['70%', '30%']\n",
      "Extracted Percentages: ['100%', '100%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['55%', '45%']\n",
      "Extracted Percentages: ['60%', '40%']\n",
      "Extracted Percentages: ['30%', '70%']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "# Load the spaCy model for NLP\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input data\n",
    "# data = [\n",
    "#     'Field size: 175 mtr x 135 mtr',\n",
    "#     'Surface: Grass',\n",
    "#     'Boundary Length (Dimension): 191 Yd x 147 Yd',\n",
    "#     'Leg Side Boundary Length: 63 meters',\n",
    "#     'Off Side Boundary Length: 72 meters',\n",
    "#     'Straight Boundary Length: 96 meters',\n",
    "#     'Back Side Boundary Length: 79 meters',\n",
    "#     'Bellerive Oval Cricket Ground is a Batting Pitch.',\n",
    "#     'Bellerive Oval Cricket Ground is a Batting Pitch. Pitches typically start off slow and gradually flatten out as the match progresses. Over time, the pitch characteristics have evolved, leading to a more balanced contest between bat and ball. While bowlers may enjoy some initial assistance, particularly early on, the conditions tend to favor the batsmen as the game moves forward, making for an exciting and competitive encounter.',\n",
    "#     'Hobart'\n",
    "# ]\n",
    "\n",
    "def processStadium(data):\n",
    "    # Regex patterns to extract relevant data\n",
    "    patterns = {\n",
    "        'field_size': r\"Field size:\\s*(\\d+\\s*mtr\\s*x\\s*\\d+\\s*mtr)\",\n",
    "        'surface': r\"Surface:\\s*(\\w+)\",\n",
    "        'boundary_length': r\"Boundary Length \\(Dimension\\):\\s*(\\d+\\s*Yd\\s*x\\s*\\d+\\s*Yd)\",\n",
    "        'leg_side_boundary': r\"Leg Side Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "        'off_side_boundary': r\"Off Side Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "        'straight_boundary': r\"Straight Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "        'back_side_boundary': r\"Back Side Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "        'pitch_type': r\"Cricket Ground is a (Batting Pitch|Bowling Pitch)\",\n",
    "        'location': r\"(\\w+)$\"\n",
    "    }\n",
    "    \n",
    "\n",
    "    # Initialize a dictionary to store the extracted values\n",
    "    extracted_data = {}\n",
    "    data_pitch_info=[]\n",
    "    # Extract values using regex\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, ' '.join(data))\n",
    "        curr_match=(match.group(1))\n",
    "        data_pitch_info.append(curr_match)\n",
    "        if match:\n",
    "            extracted_data[key] = match.group(1)\n",
    "\n",
    "    # For pitch description, you can analyze the text further if needed\n",
    "    pitch_description = [entry for entry in data if \"Pitch\" in entry]\n",
    "    if pitch_description:\n",
    "        extracted_data['pitch_description'] = ' '.join(pitch_description)\n",
    "\n",
    "    # # Output the result in JSON format\n",
    "    # json_result = json.dumps(extracted_data, indent=4)\n",
    "    # print(data_pitch_info)\n",
    "    return data_pitch_info\n",
    "\n",
    "final_data=[]\n",
    "# name=list(set(field_venue))\n",
    "stopper=0\n",
    "# type 1 error-> the way of handling web scrapping is different\n",
    "info_type1_error=[]\n",
    "for ele in venue_matches:\n",
    "    stopper+=1\n",
    "    # if(stopper==5): break\n",
    "    # try:\n",
    "    #     url = \"https://pitch-report.com/?s=\"+ele  # Replace with the actual URL\n",
    "    # # temp=[]\n",
    "    # # Send an HTTP request to fetch the page content\n",
    "    #     response = requests.get(url)\n",
    "\n",
    "    #     if response.status_code == 200:\n",
    "    #         soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    #         links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if ele in a.get_text()]\n",
    "    #         response = requests.get(links[0])\n",
    "\n",
    "    #         if response.status_code == 200:\n",
    "    #             soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    #             keywords=[\"Boundary\",\"Field\",\"Surface\"]\n",
    "    #             bounday_data=[li.get_text() for li in  soup.find_all(\"li\") if any(keyword in li.getText() for keyword in keywords)]\n",
    "\n",
    "    #             divs = soup.find_all(\"h3\")\n",
    "    #             div=[div for div in divs if \"Batting Pitch Or Bowling Pitch\" in div.get_text(strip=True)]\n",
    "\n",
    "    #             d=div[0].find_parent(\"div\").find_parent(\"div\").find_parent(\"div\").find_parent(\"div\")\n",
    "    #             d=d.find_next_sibling()\n",
    "    #             data=[p.get_text() for p in d.find_all('p')]\n",
    "    #             bounday_data.append(data[0])\n",
    "    #             temp_data=\" \".join(data)\n",
    "    #             bounday_data.append(temp_data)\n",
    "    #             bounday_data.append(ele)\n",
    "    #             final_data.append(bounday_data)\n",
    "    #             # print(final_data)\n",
    "    #             # print(bounday_data)\n",
    "    #             patterns = {\n",
    "    #                 'field_size': r\"Field size:\\s*(\\d+\\s*mtr\\s*x\\s*\\d+\\s*mtr)\",\n",
    "    #                 'surface': r\"Surface:\\s*(\\w+)\",\n",
    "    #                 'boundary_length': r\"Boundary Length \\(Dimension\\):\\s*(\\d+\\s*Yd\\s*x\\s*\\d+\\s*Yd)\",\n",
    "    #                 'leg_side_boundary': r\"Leg Side Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "    #                 'off_side_boundary': r\"Off Side Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "    #                 'straight_boundary': r\"Straight Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "    #                 'back_side_boundary': r\"Back Side Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "    #                 'pitch_type': r\"Cricket Ground is a (Batting Pitch|Bowling Pitch)\",\n",
    "    #                 'location': r\"(\\w+)$\"\n",
    "    #             }\n",
    "                \n",
    "\n",
    "    #             # Initialize a dictionary to store the extracted values\n",
    "    #             extracted_data = {}\n",
    "    #             data_pitch_info=[]\n",
    "    #             # Extract values using regex\n",
    "    #             for key, pattern in patterns.items():\n",
    "    #                 match = re.search(pattern, ' '.join(data))\n",
    "    #                 curr_match=(match.group(1))\n",
    "    #                 data_pitch_info.append(curr_match)\n",
    "    #                 if match:\n",
    "    #                     extracted_data[key] = match.group(1)\n",
    "\n",
    "    #             # For pitch description, you can analyze the text further if needed\n",
    "    #             pitch_description = [entry for entry in data if \"Pitch\" in entry]\n",
    "    #             if pitch_description:\n",
    "    #                 extracted_data['pitch_description'] = ' '.join(pitch_description)\n",
    "\n",
    "    #             print(data_pitch_info)\n",
    "    #             # break\n",
    "    # except:\n",
    "        \n",
    "    url = \"https://pitch-report.com/?s=\"+ele  # Replace with the actual URL\n",
    "# temp=[]\n",
    "# Send an HTTP request to fetch the page content\n",
    "    response = requests.get(url)\n",
    "    # print(ele)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True) if ele in a.get_text()]\n",
    "        curr_data={}\n",
    "        curr_data['venue']=ele\n",
    "        try:\n",
    "            response = requests.get(links[0])\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                # all_points=soup.find_all('ul')\n",
    "                # print(all_points)\n",
    "                all_points = soup.find_all('ul')\n",
    "                \n",
    "\n",
    "                # Filter the <ul> elements containing the word 'Floodlights'\n",
    "                # filtered_points = [ul.get_text(strip=True) for ul in all_points if 'Floodlights' in ul.get_text()]\n",
    "                # filtered_points_2 = [ul.get_text(strip=True) for ul in all_points if 'Leg Side Boundary Length' in ul.get_text()]\n",
    "                \n",
    "                # for point in filtered_points:\n",
    "                #     print(point)\n",
    "                # for point in filtered_points_2:\n",
    "                #     print(point)\n",
    "\n",
    "                # filtered_points = [li for ul in all_points for li in ul.find_all('li') if 'Floodlights' in li.get_text()]\n",
    "                # filtered_points_2 = [li for ul in all_points for li in ul.find_all('li') if 'Leg Side Boundary Length' in li.get_text()]\n",
    "\n",
    "                # # Print Floodlights-related list items\n",
    "                # print(\"Floodlights-related points:\")\n",
    "                # for li in filtered_points:\n",
    "                #     print(li.get_text(strip=True))\n",
    "\n",
    "                # # Print Boundary-related list items\n",
    "                # print(\"\\nBoundary-related points:\")\n",
    "                # for li in filtered_points_2:\n",
    "                #     print(li.get_text(strip=True))\n",
    "\n",
    "                filtered_points = [ul.find_all('li') for ul in all_points if any('Floodlights' in li.get_text() for li in ul.find_all('li'))]\n",
    "                filtered_points_2 = [ul.find_all('li') for ul in all_points if any('Leg Side Boundary Length' in li.get_text() for li in ul.find_all('li'))]\n",
    "\n",
    "                # Print all li elements from uls containing 'Floodlights'\n",
    "                # print(\"All li elements from uls containing 'Floodlights':\")\n",
    "                # print(filtered_points)\n",
    "                for li_list in filtered_points:\n",
    "                    for li in li_list:\n",
    "                        # print(li)\n",
    "                        li=li_text=li.get_text(strip=True)\n",
    "                        if('Field size:' in li):\n",
    "                            match=re.search(r\"Field size:(.+)\", li)\n",
    "                                \n",
    "                            if match:\n",
    "                                curr_data['Field size'] = match.group(1)[1:]\n",
    "                        \n",
    "                        if('Surface:' in li):\n",
    "                                \n",
    "                            match=re.search(r\"Field size:(.+)\", li)\n",
    "                                \n",
    "                            if match:\n",
    "                                curr_data['Surface'] = match.group(1)[1:]\n",
    "                                # curr_data['Surface']=re.search(r\"Surface:(.+)\", li)\n",
    "                        \n",
    "                        if('Boundary Length:' in li):\n",
    "                            if match:\n",
    "                                curr_data['Boundary Length'] = match.group(1)[1:]\n",
    "                                # curr_data['Boundary Length']=re.search(r\"Boundary Length (Dimension):(.+)\", li)\n",
    "                                \n",
    "                        # print(li.get_text(strip=True))\n",
    "\n",
    "                # # Print all li elements from uls containing 'Leg Side Boundary Length'\n",
    "                # # print(\"\\nAll li elements from uls containing 'Leg Side Boundary Length':\")\n",
    "                for li_list in filtered_points_2:\n",
    "                \n",
    "                    for li in li_list:\n",
    "                        # print(li)\n",
    "                        # li_text = str(li)  # Make sure it's a string\n",
    "                        li_text=li.get_text(strip=True)\n",
    "                        if 'Leg Side Boundary Length:' in li_text:\n",
    "                            match = re.search(r\"Leg Side Boundary Length:(.+)\", li_text)\n",
    "                            if match:\n",
    "                                curr_data['Leg Side Boundary Length'] = match.group(1)[1:]\n",
    "\n",
    "                        if 'Off Side Boundary Length:' in li_text:\n",
    "                            match = re.search(r\"Off Side Boundary Length:(.+)\", li_text)\n",
    "                            if match:\n",
    "                                curr_data['Off Side Boundary Length'] = match.group(1)[1:]\n",
    "\n",
    "                        if 'Straight Boundary Length:' in li_text:\n",
    "                            match = re.search(r\"Straight Boundary Length:(.+)\", li_text)\n",
    "                            if match:\n",
    "                                curr_data['Straight Boundary Length'] = match.group(1)[1:]\n",
    "\n",
    "                        if 'Back Side Boundary Length:' in li_text:\n",
    "                            match = re.search(r\"Back Side Boundary Length:(.+)\", li_text)\n",
    "                            if match:\n",
    "                                curr_data['Back Side Boundary Length'] = match.group(1)[1:]\n",
    "\n",
    "                # for li_list in filtered_points_2:\n",
    "                    # for li in li_list:\n",
    "                    #     li=str(li)\n",
    "                    #     if('Leg Side Boundary Length:' in li):\n",
    "                    #             curr_data['Leg Side Boundary Length']=re.search(r\"Leg Side Boundary Length:(.+)\", li)\n",
    "                    #     if('Off Side Boundary Length:' in li):      \n",
    "                    #             curr_data['Off Side Boundary Length']=re.search(r\"Off Side Boundary Length:(.+)\", li)\n",
    "                    #     if('Straight Boundary Length:' in li):\n",
    "                    #             curr_data['Straight Boundary Length']=re.search(r\"Straight Boundary Length:(.+)\", li)\n",
    "                    #     if('Back Side Boundary Length:' in li):\n",
    "                    #             curr_data['Back Side Boundary Length']=re.search(r\"Back Side Boundary Length:(.+)\", li)\n",
    "                    #             {'Leg Side Boundary Length': <re.Match object; span=(4, 34), match='Leg Side Boundary Length:</li>'>, 'Off Side Boundary Length': <re.Match object; span=(4, 34), match='Off Side Boundary Length:</li>'>, 'Straight Boundary Length': <re.Match object; span=(4, 34), match='Straight Boundary Length:</li>'>, 'Back Side Boundary Length': <re.Match object; span=(4, 35), match='Back Side Boundary Length:</li>'>}\n",
    "\n",
    "                h3_tag = soup.find('h3', string=\"Who Will Win / Match Winner Prediction\")\n",
    "                # print(h3_tag)\n",
    "                if h3_tag:\n",
    "                    # Go 3 divs out and find the next next sibling (which contains the numbers)\n",
    "                    target_div = h3_tag.find_parent('div').find_parent('div').find_parent('div').find_parent('div').find_next_sibling().find_next_sibling()\n",
    "                    # print(target_div)\n",
    "                    # Extract percentages or numbers\n",
    "                    # numbers = [div.get_text(strip=True) for div in target_div.find_all('div')]\n",
    "                    # print(\"Extracted numbers:\", numbers)\n",
    "                    percentage_spans=target_div.find_all('span', class_='elementor-progress-percentage')\n",
    "\n",
    "                    percentages = [span.get_text(strip=True) for span in percentage_spans]\n",
    "                    print(\"Extracted Percentages:\", percentages)\n",
    "                    curr_data['batting_1st']=percentages[0]\n",
    "                    curr_data['bowling_1st']=percentages[1]\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"Tag not found.\")\n",
    "\n",
    "                divs = soup.find_all(\"h3\")\n",
    "                div=[div for div in divs if \"Batting Pitch Or Bowling Pitch\" in div.get_text(strip=True)]\n",
    "\n",
    "                d=div[0].find_parent(\"div\").find_parent(\"div\").find_parent(\"div\").find_parent(\"div\")\n",
    "                d=d.find_next_sibling()\n",
    "                pitch_desc=[p.get_text() for p in d.find_all('p')]\n",
    "                # print(pitch_desc)\n",
    "                curr_data['desc']=pitch_desc\n",
    "\n",
    "            # print(curr_data)\n",
    "            info_type1_error.append(curr_data)\n",
    "        except:\n",
    "            # print({})\n",
    "            info_type1_error.append(curr_data)\n",
    "    # pass\n",
    "    # break\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_list=[]\n",
    "for entry in info_type1_error:\n",
    "    try:\n",
    "        var=entry['desc']\n",
    "        if(len(var)>1):\n",
    "            var=var[0]+\" \"+var[1]\n",
    "        else: \n",
    "            var=var[0]\n",
    "        desc_list.append(var)\n",
    "    except:\n",
    "        # print('Batting or Bowling Pitch')\n",
    "        desc_list.append('Batting or Bowling Pitch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_list\n",
    "# from claude_ai\n",
    "pitch_info=['Ba', 'Na', 'Na', 'Ba', 'Ba', 'Na', 'Na', 'Na', 'Bo', 'Na', 'Na', 'Ba', 'Na', 'Ba', 'Bo', 'Bo', 'Ba', 'Na', 'Ba', 'Bo', 'Na', 'Na', 'Na', 'Ba', 'Na', 'Ba', 'Na', 'Ba', 'Na', 'Bo', 'Bo', 'Na', 'Na', 'Ba', 'Ba', 'Na', 'Ba', 'Ba', 'Na', 'Ba', 'Ba', 'Na', 'Na', 'Na', 'Bo', 'Ba', 'Ba', 'Na', 'Na', 'Na', 'Ba', 'Na', 'Na', 'Bo', 'Na', 'Na', 'Na', 'Bo', 'Na', 'Ba', 'Na', 'Na', 'Na', 'Bo', 'Na', 'Na', 'Na', 'Na', 'Bo', 'Na', 'Na', 'Bo', 'Na', 'Ba', 'Na', 'Ba', 'Ba', 'Na', 'Na', 'Na', 'Ba', 'Na', 'Na', 'Na', 'Bo', 'Na', 'Ba', 'Na', 'Na', 'Na', 'Bo', 'Na', 'Na', 'Na', 'Na', 'Bo', 'Na', 'Na', 'Bo', 'Na', 'Ba', 'Na', 'Ba', 'Ba', 'Na', 'Na', 'Na', 'Ba', 'Na', 'Na', 'Na', 'Ba', 'Bo', 'Na', 'Ba', 'Ba', 'Ba', 'Bo', 'Na', 'Na', 'Na', 'Ba', 'Ba', 'Na', 'Na', 'Na', 'Na', 'Na', 'Na', 'Na', 'Na', 'Bo', 'Bo', 'Na', 'Na', 'Ba', 'Ba', 'Na', 'Na', 'Na', 'Ba', 'Ba', 'Na', 'Na', 'Na', 'Ba', 'Ba', 'Na', 'Bo', 'Na', 'Na', 'Na', 'Ba', 'Ba', 'Na', 'Bo', 'Na', 'Na', 'Na', 'Na', 'Na', 'Na', 'Na', 'Na', 'Ba', 'Na', 'Na', 'Na']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "temp=copy.deepcopy(info_type1_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pitch_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pitch_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(temp)):\n\u001b[0;32m----> 2\u001b[0m     temp[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpitch\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mpitch_info\u001b[49m[i]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pitch_info' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(temp)):\n",
    "    temp[i]['pitch']=pitch_info[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(temp)):\n",
    "    temp[i]['match_ids']=venue_matches[temp[i]['venue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pitch_data2.json\", \"w\") as json_file:\n",
    "    json.dump(temp, json_file, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "temp=copy.deepcopy(info_type1_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data = \"\"\"\n",
    "(Your provided text)\n",
    "\"\"\"\n",
    "\n",
    "boundary_pattern = r\"(\\w+ Side Boundary Length):\\s*(\\d+ meters|Not available)\"\n",
    "\n",
    "sets = re.split(r\"Boundary Set \\d+:\", data)\n",
    "parsed_boundaries = []\n",
    "\n",
    "for boundary_set in sets[1:]:\n",
    "    boundaries = dict(re.findall(boundary_pattern, boundary_set))\n",
    "    parsed_boundaries.append(boundaries)\n",
    "\n",
    "# Print the organized output\n",
    "for i, boundaries in enumerate(parsed_boundaries, 1):\n",
    "    print(f\"Boundary Set {i}:\")\n",
    "    for key in [\"Leg Side Boundary Length\", \"Off Side Boundary Length\", \"Straight Boundary Length\", \"Back Side Boundary Length\"]:\n",
    "        value = boundaries.get(key, \"Not available\")\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['175 mtr x 135 mtr', 'Grass', '191 Yd x 147 Yd', '63 meters', '72 meters', '96 meters', '79 meters', 'Batting Pitch', 'Hobart']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "# Load the spaCy model for NLP\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input data\n",
    "data = [\n",
    "    'Field size: 175 mtr x 135 mtr',\n",
    "    'Surface: Grass',\n",
    "    'Boundary Length (Dimension): 191 Yd x 147 Yd',\n",
    "    'Leg Side Boundary Length: 63 meters',\n",
    "    'Off Side Boundary Length: 72 meters',\n",
    "    'Straight Boundary Length: 96 meters',\n",
    "    'Back Side Boundary Length: 79 meters',\n",
    "    'Bellerive Oval Cricket Ground is a Batting Pitch.',\n",
    "    'Bellerive Oval Cricket Ground is a Batting Pitch. Pitches typically start off slow and gradually flatten out as the match progresses. Over time, the pitch characteristics have evolved, leading to a more balanced contest between bat and ball. While bowlers may enjoy some initial assistance, particularly early on, the conditions tend to favor the batsmen as the game moves forward, making for an exciting and competitive encounter.',\n",
    "    'Hobart'\n",
    "]\n",
    "\n",
    "# Regex patterns to extract relevant data\n",
    "patterns = {\n",
    "    'field_size': r\"Field size:\\s*(\\d+\\s*mtr\\s*x\\s*\\d+\\s*mtr)\",\n",
    "    'surface': r\"Surface:\\s*(\\w+)\",\n",
    "    'boundary_length': r\"Boundary Length \\(Dimension\\):\\s*(\\d+\\s*Yd\\s*x\\s*\\d+\\s*Yd)\",\n",
    "    'leg_side_boundary': r\"Leg Side Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "    'off_side_boundary': r\"Off Side Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "    'straight_boundary': r\"Straight Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "    'back_side_boundary': r\"Back Side Boundary Length:\\s*(\\d+\\s*meters)\",\n",
    "    'pitch_type': r\"Cricket Ground is a (Batting Pitch|Bowling Pitch)\",\n",
    "    'location': r\"(\\w+)$\"\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store the extracted values\n",
    "extracted_data = {}\n",
    "data_pitch_info=[]\n",
    "# Extract values using regex\n",
    "for key, pattern in patterns.items():\n",
    "    match = re.search(pattern, ' '.join(data))\n",
    "    curr_match=(match.group(1))\n",
    "    data_pitch_info.append(curr_match)\n",
    "    if match:\n",
    "        extracted_data[key] = match.group(1)\n",
    "\n",
    "# For pitch description, you can analyze the text further if needed\n",
    "pitch_description = [entry for entry in data if \"Pitch\" in entry]\n",
    "if pitch_description:\n",
    "    extracted_data['pitch_description'] = ' '.join(pitch_description)\n",
    "\n",
    "# # Output the result in JSON format\n",
    "# json_result = json.dumps(extracted_data, indent=4)\n",
    "print(data_pitch_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file_path = \"output2.csv\"\n",
    "# try:\n",
    "#     with open(file_path, mode='w', newline='') as file:\n",
    "#         writer = csv.writer(file)\n",
    "        \n",
    "#         # Write the rows of the matrix into the CSV\n",
    "#         writer.writerows(final_data)\n",
    "        \n",
    "#     print(f\"Matrix successfully written to {file_path}\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dream11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
